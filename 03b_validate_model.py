# 03c_validate_model.py
# å¢å¼ºç‰ˆæ¨¡å‹éªŒè¯å¯è§†åŒ–å·¥ - é€‚é…åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹
# åŠŸèƒ½ï¼š
# 1. åŠ è½½ best_synth_model.pt
# 2. ä» SyntheticAnimalDataset (Mode=Validation) è·å–æ ·æœ¬
# 3. å¯è§†åŒ–å¯¹æ¯”: è¾“å…¥ 2D, é¢„æµ‹ 3D, çœŸå€¼ 3D

import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Button, Slider
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append('./common')

try:
    from common.animals_dataset import AnimalsDataset
    from common.transformer_model import AnimalPoseTransformer
    from common.loss import mpjpe
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

# å¤ç”¨è®­ç»ƒè„šæœ¬ä¸­çš„ Dataset å®šä¹‰ (ä¸ºäº†ä¿è¯æ•°æ®å¤„ç†ä¸€è‡´)
# è¿™é‡Œæˆ‘ä»¬ç®€å•å¤åˆ¶ SyntheticAnimalDataset çš„éªŒè¯é€»è¾‘
# æˆ–è€…ç›´æ¥å¯¼å…¥ (å¦‚æœè®­ç»ƒè„šæœ¬æ˜¯ä½œä¸ºæ¨¡å—)ã€‚
# ä¸ºäº†ç‹¬ç«‹æ€§ï¼Œè¿™é‡Œé‡å†™ç®€åŒ–çš„éªŒè¯æ•°æ®åŠ è½½é€»è¾‘ã€‚

SKELETON_EDGES = [
    (0, 4), (4, 3), (3, 1), (3, 2), (4, 5), (5, 6), (6, 7),
    (4, 8), (8, 9), (9, 10), (0, 11), (11, 12), (12, 13), 
    (0, 14), (14, 15), (15, 16)
]

def calculate_species_scales(dataset, species_to_id):
    """è®¡ç®—æ¯ç§åŠ¨ç‰©çš„å¹³å‡éª¨éª¼é•¿åº¦ä½œä¸ºå°ºåº¦å› å­"""
    species_scales = {}
    available_animals = dataset.subjects()
    
    for animal_name, animal_id in species_to_id.items():
        if animal_name in available_animals:
            bone_lengths = []
            animal_data = dataset[animal_name]
            for action in animal_data.keys():
                positions = animal_data[action]['positions']
                if len(positions) > 0:
                    for edge in SKELETON_EDGES:
                        if edge[0] < positions.shape[1] and edge[1] < positions.shape[1]:
                            bone_vec = positions[:, edge[1]] - positions[:, edge[0]]
                            bone_len = np.linalg.norm(bone_vec, axis=-1)
                            bone_lengths.extend(bone_len)
            
            if bone_lengths:
                species_scales[animal_id] = np.mean(bone_lengths)
            else:
                species_scales[animal_id] = 1.0
        else:
            species_scales[animal_id] = 1.0
            
    return species_scales

def normalize_2d(pose_2d):
    max_vals = np.max(np.abs(pose_2d), axis=(1, 2), keepdims=True)
    max_vals[max_vals < 1e-5] = 1.0
    return pose_2d / max_vals

def batch_compute_similarity_transform_torch(S1, S2):
    """
    è®¡ç®—ä» S1 åˆ° S2 çš„åˆšä½“å˜æ¢ (Procrustes Analysis)
    S1, S2: (B, N, 3)
    è¿”å›: S1_hat (å¯¹é½åçš„ S1)
    """
    # 1. ç§»é™¤è´¨å¿ƒ
    trans1 = S1.mean(dim=1, keepdim=True)
    trans2 = S2.mean(dim=1, keepdim=True)
    S1 = S1 - trans1
    S2 = S2 - trans2

    # 3. è®¡ç®—æ—‹è½¬
    # H = S1^T * S2
    H = torch.matmul(S1.transpose(1, 2), S2) # (B, 3, 3)
    
    U, S, V = torch.svd(H)
    
    # R = V * U^T
    R = torch.matmul(V, U.transpose(1, 2))
    
    # ä¿®æ­£åå°„ (Per-sample check)
    det = torch.det(R) # (B,)
    
    # æ„å»ºå¯¹è§’çŸ©é˜µ: [1, 1, sign(det)]
    diag = torch.ones(S1.shape[0], 3, device=S1.device)
    diag[:, 2] = torch.sign(det)
    diag_mat = torch.diag_embed(diag) # (B, 3, 3)
    
    # R = V * diag * U^T
    # åªæœ‰å½“ det < 0 æ—¶æ‰éœ€è¦åè½¬
    # ä½†svdè¿”å›çš„çŸ©é˜µå¯èƒ½åŒ…å«åå°„ã€‚
    # æˆ‘ä»¬è¿™é‡Œä½¿ç”¨é€šç”¨çš„ R = V * diag * U^T
    R = torch.matmul(torch.matmul(V, diag_mat), U.transpose(1, 2))
        
    # 4. åº”ç”¨å˜æ¢
    S1_hat = torch.matmul(S1, R.transpose(1, 2))
    
    # 5.ä¸ºäº†è®¡ç®—è¯¯å·®ï¼Œéœ€è¦æŠŠ S1_hat ç§»å› S2 çš„ä½ç½®
    S1_hat = S1_hat + trans2
    
    return S1_hat

class ValidationVisualizer:
    def __init__(self, model_path, device='cuda'):
        self.device = device
        self.model = self.load_model(model_path)
        self.dataset, self.species_to_id, self.scales = self.load_data()
        self.val_samples = self.prepare_val_samples()
        
        self.current_idx = 0
        self.fig = None
        
    def load_model(self, path):
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {path}")
        model = AnimalPoseTransformer(
            num_joints=17, embed_dim=256, depth=4, num_heads=8, seq_len=27, num_species=20
        ).to(self.device)
        model.load_state_dict(torch.load(path, map_location=self.device))
        model.eval()
        return model
        
    def load_data(self):
        print("ğŸ“‚ åŠ è½½ 3D æ•°æ®...")
        dataset = AnimalsDataset('npz/real_npz/data_3d_animals.npz')
        all_animals = sorted(dataset.subjects())
        species_to_id = {name: i for i, name in enumerate(all_animals)}
        scales = calculate_species_scales(dataset, species_to_id)
        return dataset, species_to_id, scales
        
    def prepare_val_samples(self):
        print("ğŸ“¥ å‡†å¤‡éªŒè¯æ ·æœ¬...")
        samples = []
        # åªå–å‰ 5 ä¸ªåŠ¨ç‰©çš„å‰ 2 ä¸ªåŠ¨ä½œï¼Œæ¯ä¸ªå– 4 ä¸ªè§†è§’
        # é¿å…å¤ªå¤šï¼Œä½†è¿™åªæ˜¯ä¸ºäº†éšæœºæ¼«æ¸¸
        all_seqs = []
        for animal in self.dataset.subjects():
            for action in self.dataset[animal].keys():
                all_seqs.append((animal, action))
                
        # éšæœºå– 20 ä¸ªåºåˆ—ç”¨äºéªŒè¯
        import random
        random.seed(42)
        random.shuffle(all_seqs)
        selected_seqs = all_seqs[:20]
        
        for animal, action in selected_seqs:
            for view_angle in [0, 90, 180, 270]:
                samples.append({
                    'animal': animal,
                    'action': action,
                    'view': view_angle
                })
        return samples

    def get_sample_data(self, idx):
        s = self.val_samples[idx]
        animal, action, view_deg = s['animal'], s['action'], s['view']
        
        # è·å– 3D
        pos_3d_raw = self.dataset[animal][action]['positions']
        # Center crop to 27
        seq_len = 27
        if len(pos_3d_raw) >= seq_len:
            start = (len(pos_3d_raw) - seq_len) // 2
            pos_3d = pos_3d_raw[start:start+seq_len]
        else:
            pad = seq_len - len(pos_3d_raw)
            pos_3d = np.pad(pos_3d_raw, ((0, pad), (0,0), (0,0)), mode='edge')
            
        pos_3d = pos_3d - pos_3d[:, 0:1, :] # Root rel
        
        # æ—‹è½¬ (ç”Ÿæˆ Input)
        theta = np.deg2rad(view_deg)
        c, s_sin = np.cos(theta), np.sin(theta)
        Ry = np.array([[c, 0, s_sin], [0, 1, 0], [-s_sin, 0, c]], dtype=np.float32)
        pos_3d_rot = np.matmul(pos_3d, Ry.T)
        
        # æŠ•å½± 2D
        pos_2d = pos_3d_rot[..., [0, 2]] # X, Z
        pos_2d_norm = normalize_2d(pos_2d)
        
        return {
            'input_2d': torch.tensor(pos_2d_norm, dtype=torch.float32).unsqueeze(0).to(self.device),
            'gt_3d': torch.tensor(pos_3d, dtype=torch.float32).unsqueeze(0).to(self.device), # Canonical
            'gt_3d_rot': torch.tensor(pos_3d_rot, dtype=torch.float32).to(self.device), # Rotated (Camera Space)
            'species_id': torch.tensor([self.species_to_id[animal]], device=self.device),
            'meta': s
        }

    def visualize(self):
        import matplotlib
        matplotlib.use('TkAgg')
        
        self.fig = plt.figure(figsize=(18, 8))
        self.ax1 = self.fig.add_subplot(131, title="Input 2D View")
        self.ax2 = self.fig.add_subplot(132, projection='3d', title="Prediction (Aligned)")
        self.ax3 = self.fig.add_subplot(133, projection='3d', title="Ground Truth (Canonical)")
        
        plt.subplots_adjust(bottom=0.2)
        
        self.btn_prev = Button(plt.axes([0.3, 0.05, 0.1, 0.075]), 'Previous')
        self.btn_next = Button(plt.axes([0.6, 0.05, 0.1, 0.075]), 'Next')
        
        self.btn_prev.on_clicked(self.prev_sample)
        self.btn_next.on_clicked(self.next_sample)
        
        self.update_plot()
        plt.show()
        
    def prev_sample(self, event):
        self.current_idx = (self.current_idx - 1) % len(self.val_samples)
        self.update_plot()
        
    def next_sample(self, event):
        self.current_idx = (self.current_idx + 1) % len(self.val_samples)
        self.update_plot()
        
    def update_plot(self):
        data = self.get_sample_data(self.current_idx)
        
        # Inference
        with torch.no_grad():
            pred_norm = self.model(data['input_2d'], data['species_id'])
            scale = self.scales[data['species_id'].item()]
            pred_3d = pred_norm * scale
            
        # Post-process for visu (Take 1st frame or middle frame)
        frame_idx = 13 # Middle
        
        p2 = data['input_2d'][0, frame_idx].cpu().numpy()
        p3_pred = pred_3d[0, frame_idx].cpu().numpy()
        p3_gt = data['gt_3d'][0, frame_idx].cpu().numpy()
        p3_gt_rot = data['gt_3d_rot'][frame_idx].cpu().numpy()
        
        # Alignment (PA-MPJPE logic for viz)
        # Align Pred to GT (Canonical) for fair visual comparison
        # æ³¨æ„: æ¨¡å‹è¾“å‡ºçš„æ˜¯ Camera Space (Rotated)ï¼ŒGT æ˜¯ Canonicalã€‚
        # å¦‚æœæˆ‘ä»¬ç›´æ¥ç”» Predï¼Œå®ƒæ˜¯æ­ªçš„ï¼ˆå¯¹äº 90åº¦è§†è§’ï¼‰ã€‚
        # ä¸ºäº†éªŒè¯ "Pose" å¯¹ä¸å¯¹ï¼Œæˆ‘ä»¬æŠŠ Pred å¯¹é½åˆ° GTã€‚
        p3_pred_aligned = batch_compute_similarity_transform_torch(
            torch.tensor(p3_pred).unsqueeze(0), 
            torch.tensor(p3_gt).unsqueeze(0)
        ).squeeze(0).numpy()
        
        # 1. 2D Input
        self.ax1.clear()
        self.ax1.set_title(f"Input 2D (View {data['meta']['view']}Â°)")
        self.draw_2d_skeleton(self.ax1, p2)
        self.ax1.set_aspect('equal')
        self.ax1.invert_yaxis() # Image coord
        
        # 2. Prediction (White) vs GT (Green) [Aligned]
        self.ax2.clear()
        self.ax2.set_title(f"Pred (Aligned) vs GT\nAnimal: {data['meta']['animal']}")
        self.draw_3d_skeleton(self.ax2, p3_gt, 'green', 'GT')
        self.draw_3d_skeleton(self.ax2, p3_pred_aligned, 'red', 'Pred')
        self.set_3d_axes(self.ax2, p3_gt)
        self.ax2.legend()
        
        # 3. Raw Prediction (Camera Space)
        self.ax3.clear()
        self.ax3.set_title(f"Raw Output (Camera Space)\nShould match View Angle")
        self.draw_3d_skeleton(self.ax3, p3_pred, 'blue', 'RawPred')
        # ç”»ä¸€ä¸ªå‚ç…§çš„ Ground Plane æˆ– Axis æŒ‡ç¤ºæ–¹å‘
        self.ax3.quiver(0,0,0, 100,0,0, color='r', arrow_length_ratio=0.1) # X
        self.ax3.quiver(0,0,0, 0,100,0, color='g', arrow_length_ratio=0.1) # Y
        self.ax3.quiver(0,0,0, 0,0,100, color='b', arrow_length_ratio=0.1) # Z
        self.set_3d_axes(self.ax3, p3_pred)
        
        self.fig.canvas.draw_idle()

    def draw_2d_skeleton(self, ax, pose):
        for s, e in SKELETON_EDGES:
            ax.plot([pose[s,0], pose[e,0]], [pose[s,1], pose[e,1]], 'b-')
        ax.scatter(pose[:,0], pose[:,1], c='r', s=10)

    def draw_3d_skeleton(self, ax, pose, color, label):
        first = True
        for s, e in SKELETON_EDGES:
            ax.plot([pose[s,0], pose[e,0]], [pose[s,1], pose[e,1]], [pose[s,2], pose[e,2]], color=color, label=label if first else "")
            first = False
        ax.scatter(pose[:,0], pose[:,1], pose[:,2], c=color, s=10)
        
    def set_3d_axes(self, ax, pose):
        limit = np.max(np.abs(pose)) * 1.5
        ax.set_xlim(-limit, limit)
        ax.set_ylim(-limit, limit)
        ax.set_zlim(-limit, limit)
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')

if __name__ == '__main__':
    checkpoint = 'checkpoints/best_synth_model.pt'
    if not os.path.exists(checkpoint):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹: {checkpoint}")
    else:
        viz = ValidationVisualizer(checkpoint)
        viz.visualize()