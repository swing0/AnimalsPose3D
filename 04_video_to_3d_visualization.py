# 04b_video_to_3d_visualization.py
# è§†é¢‘åˆ°3Då…³é”®ç‚¹çš„å¯è§†åŒ–å·¥å…· - é€‚é…æœ€ä½³åˆæˆæ¨¡å‹ (Best Synth Model)

import os
import cv2
import numpy as np
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
from matplotlib.widgets import Slider, Button
from tqdm import tqdm
import sys

# æ·»åŠ è·¯å¾„
sys.path.append('./common')

try:
    from common.ap10k_detector import AP10KAnimalPoseDetector
    from common.keypoint_mapper import KeypointMapper
    from common.transformer_model import AnimalPoseTransformer
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

# éª¨æ¶åˆ†ç»„å®šä¹‰
SKELETON_GROUPS = {
    'trunk': {'edges': [(0, 4), (4, 3), (3, 1), (3, 2)], 'color': 'black', 'label': 'Head & Neck'},
    'front_left': {'edges': [(4, 5), (5, 6), (6, 7)], 'color': 'red', 'label': 'Front Left'},
    'front_right': {'edges': [(4, 8), (8, 9), (9, 10)], 'color': 'orange', 'label': 'Front Right'},
    'back_left': {'edges': [(0, 11), (11, 12), (12, 13)], 'color': 'blue', 'label': 'Back Left'},
    'back_right': {'edges': [(0, 14), (14, 15), (15, 16)], 'color': 'cyan', 'label': 'Back Right'}
}

SKELETON_COLORS_2D = {
    'trunk': (0, 0, 0), 'front_left': (0, 0, 255), 'front_right': (0, 165, 255),
    'back_left': (255, 0, 0), 'back_right': (255, 255, 0)
}

class VideoTo3DVisualizer:
    def __init__(self, model_checkpoint, onnx_model_path, target_species_id=0):
        print("ğŸ¯ åˆå§‹åŒ–è§†é¢‘åˆ°3Då¯è§†åŒ–å™¨...")
        self.detector = AP10KAnimalPoseDetector(onnx_model_path)
        self.mapper = KeypointMapper()
        self.target_species_id = target_species_id
        
        # Load Model
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_3d = self.load_3d_model(model_checkpoint)
        
        self.video_frames = []
        self.keypoints_2d_sequence = []
        self.keypoints_3d_sequence = []
        
        print("âœ… å¯è§†åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_3d_model(self, checkpoint_path):
        print(f"ğŸ“¥ åŠ è½½3Dæ¨¡å‹: {checkpoint_path}")
        # å‚æ•°å¿…é¡»ä¸ 03c_train_enhanced.py ä¸€è‡´
        model = AnimalPoseTransformer(
            num_joints=17, 
            embed_dim=256, 
            depth=4, 
            num_heads=8, 
            seq_len=27, 
            num_species=20
        ).to(self.device)
        
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return None
            
        try:
            state_dict = torch.load(checkpoint_path, map_location=self.device)
            model.load_state_dict(state_dict)
            model.eval()
            print(f"âœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
            
        return model
    
    def extract_video_frames(self, video_path, max_frames=300):
        print(f"ğŸ¥ å¤„ç†è§†é¢‘: {video_path}")
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened(): raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if max_frames: total_frames = min(total_frames, max_frames)
        
        self.video_frames = []
        self.keypoints_2d_sequence = []
        
        pbar = tqdm(total=total_frames, desc="æå–è§†é¢‘å’Œå…³é”®ç‚¹")
        for i in range(total_frames):
            ret, frame = cap.read()
            if not ret: break
            
            self.video_frames.append(frame.copy())
            
            # Detect
            # ä¸ºäº†é€Ÿåº¦ï¼Œå¯ä»¥æ¯éš”å‡ å¸§æ£€æµ‹ä¸€æ¬¡ç„¶åæ’å€¼ï¼Œè¿™é‡Œæ¼”ç¤ºé€å¸§
            temp_path = f"temp_{i}.jpg"
            cv2.imwrite(temp_path, frame)
            try:
                result = self.detector.predict(temp_path)
                kps = result['keypoints']
                if np.sum(kps[:, 2] > 0.3) >= 8:
                    kps_train = self.mapper.map_ap10k_to_training(kps)
                    self.keypoints_2d_sequence.append(kps_train[:, :2])
                else:
                    self.keypoints_2d_sequence.append(np.full((17, 2), np.nan))
            except:
                self.keypoints_2d_sequence.append(np.full((17, 2), np.nan))
                
            if os.path.exists(temp_path): os.remove(temp_path)
            pbar.update(1)
        pbar.close()
        cap.release()
        return len(self.video_frames)

    def normalize_root_relative(self, kps_seq):
        """
        å½’ä¸€åŒ–é€»è¾‘ (ä¸ SyntheticAnimalDataset è®­ç»ƒæ—¶ä¸€è‡´):
        1. Root Relative (å‡å»æ ¹èŠ‚ç‚¹)
        2. Scale Normalization (é™¤ä»¥æœ€å¤§ç»å¯¹å€¼)
        """
        normalized_seq = []
        stored_scales = [] # ç”¨äºåå½’ä¸€åŒ– (Visualization purpose)
        
        for kps in kps_seq:
            if np.any(np.isnan(kps)):
                normalized_seq.append(kps) # Keep NaN
                stored_scales.append(1.0)
                continue
                
            # 1. Root Relative
            # å‡è®¾ç¬¬0ä¸ªå…³èŠ‚æ˜¯æ ¹èŠ‚ç‚¹ (Hip/Pelvis)
            root = kps[0:1, :] 
            kps_centered = kps - root
            
            # 2. Scale
            max_val = np.max(np.abs(kps_centered))
            if max_val < 1e-5: max_val = 1.0
            
            kps_norm = kps_centered / max_val
            
            # Flip Y to match Model's Up-positive convention (Image is Down-positive)
            kps_norm[:, 1] *= -1
            
            normalized_seq.append(kps_norm)
            stored_scales.append(max_val)
            
        return np.array(normalized_seq), stored_scales

    def convert_2d_to_3d(self):
        print("ğŸ”„ è½¬æ¢ 2D -> 3D...")
        if not self.keypoints_2d_sequence: return False
        
        kps_2d_arr = np.array(self.keypoints_2d_sequence)
        kps_norm_arr, scales = self.normalize_root_relative(kps_2d_arr)
        
        self.keypoints_3d_sequence = []
        seq_len = 27
        
        # æ»‘åŠ¨çª—å£å¤„ç†
        # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬é€å¸§æ»‘åŠ¨ï¼Œæˆ–è€…åˆ†å—
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„ padding ç­–ç•¥
        
        num_frames = len(kps_norm_arr)
        
        # Pad beginning
        pad_len = seq_len // 2
        padded_input = np.pad(kps_norm_arr, ((pad_len, pad_len), (0,0), (0,0)), mode='edge')
        
        # Inference Loop
        bs = 32
        
        # å‡†å¤‡ batch
        input_batches = []
        indices = []
        
        current_batch = []
        
        for i in range(num_frames):
            # å–çª—å£ [i : i+seq_len]
            # å¯¹åº”çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸­é—´å¸§ï¼Œæˆ–è€…æœ€åä¸€å¸§ï¼Ÿ
            # è®­ç»ƒæ˜¯ seq -> all joints 3d.
            # ä¸ºäº†å¹³æ»‘ï¼Œæˆ‘ä»¬é€šå¸¸å– centered window.
            # padding ä¹‹åï¼Œç¬¬ i å¸§ å¯¹åº” window [i : i+seq_len] (centered at i + pad_len)
            
            window = padded_input[i : i+seq_len]
            if np.any(np.isnan(window)):
                # å¦‚æœçª—å£å†…æœ‰ NaNï¼Œè¿™ä¸€å¸§å¯èƒ½ä¸å‡†ï¼Œä½†è¿˜æ˜¯å¾—é¢„æµ‹
                # fill nan with 0
                window = np.nan_to_num(window, 0.0)
                
            current_batch.append(window)
            if len(current_batch) == bs or i == num_frames - 1:
                input_batches.append(np.array(current_batch))
                current_batch = []

        # Run Model
        all_preds = []
        with torch.no_grad():
            for batch_np in tqdm(input_batches, desc="3D æ¨ç†"):
                batch_tensor = torch.tensor(batch_np, dtype=torch.float32).to(self.device)
                
                # Species ID
                species = torch.full((batch_tensor.shape[0],), self.target_species_id, dtype=torch.long).to(self.device)
                
                # Forward
                pred_norm = self.model_3d(batch_tensor, species) # (B, 27, 17, 3)
                
                # å–ä¸­é—´å¸§è¿˜æ˜¯æœ€åä¸€å¸§ï¼Ÿ
                # è®­ç»ƒæ—¶å¯ä»¥å–åºåˆ—ã€‚
                # Transformer è¾“å‡ºä¹Ÿæ˜¯åºåˆ— (B, T, J, 3)
                # æˆ‘ä»¬å–ä¸­é—´å¸§ pad_len
                pred_frame = pred_norm[:, seq_len // 2, :, :] # (B, 17, 3)
                
                all_preds.append(pred_frame.cpu().numpy())
                
        all_preds = np.concatenate(all_preds, axis=0) # (N, 17, 3)
        
        # åå½’ä¸€åŒ– (ä»… Scaleï¼Œå› ä¸ºæ˜¯ Root Relative çš„ 3D)
        # 3D é¢„æµ‹ä¹Ÿæ˜¯ Root Relative çš„
        # æˆ‘ä»¬ç»™å®ƒä¹˜ä»¥ä¸€ä¸ªç³»æ•°ï¼Œè®©å®ƒçœ‹èµ·æ¥å¤§ä¸€ç‚¹
        # æˆ–è€…ä¹˜ä»¥åŸå§‹ 2D çš„ scale
        
        for i, pred_3d_norm in enumerate(all_preds):
            scale = scales[i] if i < len(scales) else 100.0
            if scale == 1.0: scale = 100.0 # Default if unknown
            
            # ä¹˜ä»¥ Scale è¿˜åŸç‰©ç†å¤§å° (Approx)
            # è¿™é‡Œçš„ Scale æ˜¯ 2D pixel scaleã€‚
            # ç›´æ¥ç”¨å®ƒä¼šè®© 3D çœ‹èµ·æ¥å’Œ 2D åƒç´ ç©ºé—´å¯¹åº”ã€‚
            pred_3d = pred_3d_norm * scale * 2.0 # 2.0 æ˜¯ç»éªŒç³»æ•° (Z-depth usually larger)
            
            self.keypoints_3d_sequence.append(pred_3d)
            
        print(f"âœ… 3Dåºåˆ—ç”Ÿæˆå®Œæ¯•: {len(self.keypoints_3d_sequence)} å¸§")
        return True

    def visualize_combined(self, video_path):
        if not self.extract_video_frames(video_path): return
        if not self.convert_2d_to_3d(): return
        
        print("ğŸ¬ å¯åŠ¨å¯è§†åŒ–ç•Œé¢...")
        import matplotlib
        matplotlib.use('TkAgg')
        
        fig = plt.figure(figsize=(18, 9))
        ax_2d = fig.add_subplot(121)
        ax_3d = fig.add_subplot(122, projection='3d')
        
        ax_2d.axis('off')
        ax_2d.set_title("Input Video")
        ax_3d.set_title("3D Reconstruction")
        
        # Plot objects
        img_plot = None
        scatter_3d = None
        lines_3d = []
        
        # Axis limits
        valid_3d = [p for p in self.keypoints_3d_sequence if not np.any(np.isnan(p))]
        if valid_3d:
            valid_3d = np.array(valid_3d)
            limit = np.max(np.abs(valid_3d)) * 1.2
            ax_3d.set_xlim(-limit, limit)
            ax_3d.set_ylim(-limit, limit)
            ax_3d.set_zlim(-limit, limit)
        
        ax_3d.view_init(elev=20, azim=45)
        
        def update(frame_idx):
            nonlocal img_plot, scatter_3d, lines_3d
            
            # 2D
            if frame_idx < len(self.video_frames):
                frame = self.video_frames[frame_idx].copy()
                kps = self.keypoints_2d_sequence[frame_idx]
                
                # Draw Skeleton
                if not np.any(np.isnan(kps)):
                    for s, e in list(SKELETON_GROUPS['trunk']['edges']) + list(SKELETON_GROUPS['front_left']['edges']) + \
                                list(SKELETON_GROUPS['front_right']['edges']) + list(SKELETON_GROUPS['back_left']['edges']) + \
                                list(SKELETON_GROUPS['back_right']['edges']):
                        if s < len(kps) and e < len(kps):
                            pt1 = (int(kps[s][0]), int(kps[s][1]))
                            pt2 = (int(kps[e][0]), int(kps[e][1]))
                            cv2.line(frame, pt1, pt2, (0, 255, 0), 2)
                            
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                if img_plot is None:
                    img_plot = ax_2d.imshow(frame)
                else:
                    img_plot.set_data(frame)
            
            # 3D
            if frame_idx < len(self.keypoints_3d_sequence):
                pose = self.keypoints_3d_sequence[frame_idx]
                if scatter_3d is not None: scatter_3d.remove()
                for l in lines_3d: l.remove()
                lines_3d = []
                
                if not np.any(np.isnan(pose)):
                    # Rotate for better view (Optional)
                    # pose = pose @ R_x(np.pi/2) ...
                    
                    scatter_3d = ax_3d.scatter(pose[:,0], pose[:,1], pose[:,2], c='r', s=20)
                    
                    for group_name, info in SKELETON_GROUPS.items():
                        for s, e in info['edges']:
                             lines_3d.append(ax_3d.plot(
                                 [pose[s,0], pose[e,0]],
                                 [pose[s,1], pose[e,1]],
                                 [pose[s,2], pose[e,2]],
                                 color=info['color']
                             )[0])
                             
            ax_2d.set_title(f"Frame {frame_idx}")

        total_frames = len(self.video_frames)
        ani = animation.FuncAnimation(fig, update, frames=total_frames, interval=50)
        
        plt.show()

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--video', type=str, default='video/luotuo.mp4', help='Path to video')
    parser.add_argument('--species_id', type=int, default=0, help='Species ID (0-19)')
    args = parser.parse_args()
    
    checkpoint = 'checkpoints_enhanced/best_synth_model.pt'
    onnx_path = 'model/ap10k/end2end.onnx'
    
    if not os.path.exists(args.video):
        print(f"Please provide valid video path. {args.video} not found.")
        # Try finding a video
        if os.path.exists("video"):
            vids = os.listdir("video")
            if vids:
                args.video = os.path.join("video", vids[0])
                print(f"Using found video: {args.video}")
    
    viz = VideoTo3DVisualizer(checkpoint, onnx_path, target_species_id=args.species_id)
    if viz.model_3d:
        viz.visualize_combined(args.video)

if __name__ == '__main__':
    main()