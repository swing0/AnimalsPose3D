import torch
import torch.nn as nn
import torch.nn.functional as F


class UltraLightAnimalPoseTransformer(nn.Module):
    """
    è¶…è½»é‡åŠ¨ç‰©å§¿æ€Transformer - ä¸“ä¸º8GBæ˜¾å­˜ä¼˜åŒ–
    å…³é”®ä¼˜åŒ–ï¼š
    1. æå°çš„æ¨¡å‹å°ºå¯¸
    2. å‡å°‘åºåˆ—é•¿åº¦å’Œæ³¨æ„åŠ›è®¡ç®—
    3. æ··åˆç²¾åº¦è®­ç»ƒå‹å¥½
    """
    
    def __init__(self, num_joints=17, in_dim=2, embed_dim=96, 
                 depth=2, num_heads=4, seq_len=16, dropout=0.1):
        """
        å‚æ•°ï¼š
            num_joints: å…³èŠ‚æ•°é‡ (é»˜è®¤17)
            in_dim: è¾“å…¥ç»´åº¦ (2Dåæ ‡=2)
            embed_dim: åµŒå…¥ç»´åº¦ (å¤§å¹…å‡å°‘)
            depth: Transformerå±‚æ•° (å¤§å¹…å‡å°‘)
            num_heads: æ³¨æ„åŠ›å¤´æ•° (å‡å°‘)
            seq_len: åºåˆ—é•¿åº¦ (å¿…é¡»<=16)
            dropout: Dropoutç‡
        """
        super().__init__()
        
        # ä¿å­˜å‚æ•°
        self.num_joints = num_joints
        self.seq_len = seq_len
        self.embed_dim = embed_dim
        
        # 1. å…³èŠ‚ç‰¹å¾åµŒå…¥ (æç®€)
        self.joint_embed = nn.Linear(in_dim, embed_dim)
        
        # 2. ä½ç½®ç¼–ç  (å­¦ä¹ å¼)
        self.time_pos_embed = nn.Parameter(torch.randn(1, seq_len, 1, embed_dim) * 0.02)
        self.joint_pos_embed = nn.Parameter(torch.randn(1, 1, num_joints, embed_dim) * 0.02)
        
        # 3. æç®€Transformerç¼–ç å™¨ (2å±‚)
        self.transformer_layers = nn.ModuleList([
            self._create_transformer_layer(embed_dim, num_heads, dropout)
            for _ in range(depth)
        ])
        
        # 4. è¾“å‡ºå±‚ (æç®€)
        self.norm = nn.LayerNorm(embed_dim)
        self.output_proj = nn.Linear(embed_dim, 3)  # ç›´æ¥è¾“å‡º3Dåæ ‡
        
        # åˆå§‹åŒ–
        self._init_weights()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.parameters())
        print(f"ğŸ”§ è¶…è½»é‡æ¨¡å‹åˆ›å»ºå®Œæˆ:")
        print(f"  å‚æ•°é‡: {total_params:,}")
        print(f"  åºåˆ—é•¿åº¦: {seq_len}")
        print(f"  åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"  å±‚æ•°: {depth}")
        print(f"  æ³¨æ„åŠ›å¤´: {num_heads}")
    
    def _create_transformer_layer(self, embed_dim, num_heads, dropout):
        """åˆ›å»ºè½»é‡Transformerå±‚"""
        return nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 2,  # å°FFN
            dropout=dropout,
            batch_first=True,
            activation='relu',  # ä½¿ç”¨ReLUèŠ‚çœæ˜¾å­˜
            norm_first=True  # å…ˆå½’ä¸€åŒ–æ›´ç¨³å®š
        )
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # çº¿æ€§å±‚ä½¿ç”¨Xavieråˆå§‹åŒ–
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
        # ä½ç½®ç¼–ç å·²éšæœºåˆå§‹åŒ–
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥: (B, T, J, 2) - 2Då…³èŠ‚ç‚¹åæ ‡
        è¾“å‡º: (B, T, J, 3) - 3Då…³èŠ‚ç‚¹åæ ‡
        """
        batch_size, seq_len, num_joints, _ = x.shape
        
        # æ£€æŸ¥è¾“å…¥åºåˆ—é•¿åº¦
        if seq_len > self.seq_len:
            raise ValueError(f"è¾“å…¥åºåˆ—é•¿åº¦{seq_len}è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦{self.seq_len}")
        
        # 1. å…³èŠ‚ç‰¹å¾åµŒå…¥
        x = self.joint_embed(x)  # (B, T, J, D)
        
        # 2. æ·»åŠ ä½ç½®ç¼–ç  (å¹¿æ’­)
        x = x + self.time_pos_embed[:, :seq_len, :, :]
        x = x + self.joint_pos_embed[:, :, :num_joints, :]
        
        # 3. é‡å¡‘ä¸ºåºåˆ—: (B, T*J, D)
        x = x.reshape(batch_size, seq_len * num_joints, self.embed_dim)
        
        # 4. é€šè¿‡Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x)
        
        # 5. å±‚å½’ä¸€åŒ–
        x = self.norm(x)
        
        # 6. æ¢å¤åŸå§‹å½¢çŠ¶
        x = x.reshape(batch_size, seq_len, num_joints, self.embed_dim)
        
        # 7. è¾“å‡º3Dåæ ‡
        x = self.output_proj(x)
        
        return x


class TinyAnimalPoseTransformer(nn.Module):
    """
    è¶…å°æ¨¡å‹ - å¦‚æœä¸Šé¢çš„æ¨¡å‹è¿˜æ˜¯å¤ªå¤§
    ä½¿ç”¨å·ç§¯+æ³¨æ„åŠ›æ··åˆæ¶æ„
    """
    def __init__(self, num_joints=17, seq_len=16, hidden_dim=64):
        super().__init__()
        
        # 1. å·ç§¯ç¼–ç å™¨ (èŠ‚çœæ˜¾å­˜)
        self.conv_encoder = nn.Sequential(
            nn.Conv2d(2, 32, kernel_size=(3, 3), padding=1),  # (B, 32, T, J)
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((seq_len, num_joints))
        )
        
        # 2. è½»é‡æ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=2,
            dropout=0.1,
            batch_first=True
        )
        
        # 3. è¾“å‡ºå±‚
        self.output = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 3)
        )
        
        print(f"ğŸ”§ è¶…å°æ¨¡å‹åˆ›å»ºå®Œæˆ: {sum(p.numel() for p in self.parameters()):,}å‚æ•°")
    
    def forward(self, x):
        # x: (B, T, J, 2)
        b, t, j, _ = x.shape
        
        # è½¬ç½®ä¸ºå·ç§¯æ ¼å¼
        x = x.permute(0, 3, 1, 2)  # (B, 2, T, J)
        x = self.conv_encoder(x)
        x = x.permute(0, 2, 3, 1)  # (B, T, J, 64)
        
        # é‡å¡‘å¹¶åº”ç”¨æ³¨æ„åŠ›
        x = x.reshape(b, t * j, -1)
        x, _ = self.attention(x, x, x)
        
        # æ¢å¤å½¢çŠ¶å¹¶è¾“å‡º
        x = x.reshape(b, t, j, -1)
        x = self.output(x)
        
        return x


def test_model_memory():
    """æµ‹è¯•æ¨¡å‹æ˜¾å­˜å ç”¨"""
    import time
    
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹æ˜¾å­˜å ç”¨...")
    
    # æµ‹è¯•é…ç½®
    batch_sizes = [2, 4, 8]
    seq_lens = [8, 16]
    
    for seq_len in seq_lens:
        for batch_size in batch_sizes:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            try:
                # åˆ›å»ºæ¨¡å‹
                model = UltraLightAnimalPoseTransformer(
                    num_joints=17,
                    embed_dim=96,
                    seq_len=seq_len
                ).cuda()
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(batch_size, seq_len, 17, 2).cuda()
                
                # å‰å‘ä¼ æ’­
                output = model(test_input)
                
                # åˆ›å»ºè™šæ‹ŸæŸå¤±å¹¶åå‘ä¼ æ’­
                loss = output.mean()
                loss.backward()
                
                # è·å–æ˜¾å­˜ä½¿ç”¨
                peak_memory = torch.cuda.max_memory_allocated() / (1024**2)
                allocated = torch.cuda.memory_allocated() / (1024**2)
                reserved = torch.cuda.memory_reserved() / (1024**2)
                
                print(f"  batch={batch_size}, seq={seq_len}: "
                      f"å³°å€¼{peak_memory:.1f}MB, "
                      f"å·²åˆ†é…{allocated:.1f}MB, "
                      f"ä¿ç•™{reserved:.1f}MB")
                
                del model, test_input, output, loss
                torch.cuda.empty_cache()
                
            except torch.cuda.OutOfMemoryError:
                print(f"  âŒ batch={batch_size}, seq={seq_len}: OOMé”™è¯¯")
            
            time.sleep(0.5)
    
    print("æµ‹è¯•å®Œæˆ!\n")


if __name__ == '__main__':
    # è¿è¡Œæ˜¾å­˜æµ‹è¯•
    if torch.cuda.is_available():
        test_model_memory()
    else:
        print("æ²¡æœ‰å¯ç”¨çš„GPUï¼Œè·³è¿‡æ˜¾å­˜æµ‹è¯•")