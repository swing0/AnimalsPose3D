import torch
import torch.nn as nn
import torch.nn.functional as F


class AnimalPoseTransformer(nn.Module):
    """
    å¢å¼ºç‰ˆåŠ¨ç‰©å§¿æ€Transformer - ä¸“ä¸º8GBæ˜¾å­˜ä¼˜åŒ–ä½†æ€§èƒ½æ›´å¼º
    å…³é”®æ”¹è¿›ï¼š
    1. å¢åŠ æ¨¡å‹å®¹é‡ (embed_dim=256, depth=4)
    2. ä»2Då…³é”®ç‚¹è‡ªåŠ¨æå–å½¢æ€ç‰¹å¾ (æ— éœ€ç‰©ç§ID)
    3. æ”¯æŒæ›´é•¿çš„åºåˆ— (seq_len=27)
    4. ä¼˜åŒ–æŸå¤±å‡½æ•°
    """
    
    def __init__(self, num_joints=17, in_dim=2, embed_dim=256, 
                 depth=4, num_heads=8, seq_len=27, dropout=0.1):
        """
        å‚æ•°ï¼š
            num_joints: å…³èŠ‚æ•°é‡ (é»˜è®¤17)
            in_dim: è¾“å…¥ç»´åº¦ (2Dåæ ‡=2)
            embed_dim: åµŒå…¥ç»´åº¦ (å¢åŠ åˆ°256)
            depth: Transformerå±‚æ•° (å¢åŠ åˆ°4)
            num_heads: æ³¨æ„åŠ›å¤´æ•° (å¢åŠ åˆ°8)
            seq_len: åºåˆ—é•¿åº¦ (å¢åŠ åˆ°27)
            dropout: Dropoutç‡
        """
        super().__init__()
        
        # ä¿å­˜å‚æ•°
        self.num_joints = num_joints
        self.seq_len = seq_len
        self.embed_dim = embed_dim
        self.in_dim = in_dim
        
        # 1. ä»2Då…³é”®ç‚¹æå–å…¨å±€å½¢æ€ç‰¹å¾
        # è¿™å°†è‡ªåŠ¨å­¦ä¹ ç‰©ç§ç‰¹å®šçš„å½¢æ€ç‰¹å¾ï¼ˆä½“å‹æ¯”ä¾‹ã€è‚¢ä½“é•¿åº¦ç­‰ï¼‰
        self.pose_feature_extractor = nn.Sequential(
            nn.Linear(num_joints * in_dim, embed_dim),
            nn.ReLU(),
            nn.Linear(embed_dim, embed_dim)
        )
        
        # 2. å…³èŠ‚ç‰¹å¾åµŒå…¥
        self.joint_embed = nn.Linear(in_dim, embed_dim)
        
        # 3. ä½ç½®ç¼–ç  (å­¦ä¹ å¼)
        self.time_pos_embed = nn.Parameter(torch.randn(1, seq_len, 1, embed_dim) * 0.02)
        
        # å¢å¼ºä½ç½®ç¼–ç ï¼šä¸ºå·¦å³å…³èŠ‚æ³¨å…¥ä¸åŒçš„åç½®å€¼
        joint_pos_embed = torch.randn(1, 1, num_joints, embed_dim) * 0.02
        
        # AP10Kå…³é”®ç‚¹å®šä¹‰ï¼šå·¦ä¾§å…³èŠ‚ [1, 2, 5, 6, 7, 11, 12, 13]
        # å³ä¾§å…³èŠ‚ [3, 4, 8, 9, 10, 14, 15, 16]
        left_joints = [1, 2, 5, 6, 7, 11, 12, 13]
        right_joints = [3, 4, 8, 9, 10, 14, 15, 16]
        
        # ä¸ºå·¦ä¾§å…³èŠ‚æ·»åŠ æ­£åç½®ï¼Œå³ä¾§å…³èŠ‚æ·»åŠ è´Ÿåç½®
        for j in left_joints:
            if j < num_joints:
                joint_pos_embed[0, 0, j, :] += 0.1  # å·¦ä¾§åç½®
        
        for j in right_joints:
            if j < num_joints:
                joint_pos_embed[0, 0, j, :] -= 0.1  # å³ä¾§åç½®
        
        self.joint_pos_embed = nn.Parameter(joint_pos_embed)
        
        # 4. å¢å¼ºTransformerç¼–ç å™¨ (4å±‚)
        self.transformer_layers = nn.ModuleList([
            self._create_transformer_layer(embed_dim, num_heads, dropout)
            for _ in range(depth)
        ])
        
        # 5. è¾“å‡ºå±‚
        self.norm = nn.LayerNorm(embed_dim)
        self.output_proj = nn.Linear(embed_dim, 3)  # ç›´æ¥è¾“å‡º3Dåæ ‡
        

        # 6. åˆå§‹åŒ–
        self._init_weights()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.parameters())
        print(f"ğŸ”§ å¢å¼ºç‰ˆæ¨¡å‹åˆ›å»ºå®Œæˆ:")
        print(f"  å‚æ•°é‡: {total_params:,}")
        print(f"  åºåˆ—é•¿åº¦: {seq_len}")
        print(f"  åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"  å±‚æ•°: {depth}")
        print(f"  æ³¨æ„åŠ›å¤´: {num_heads}")
        print(f"  ç‰¹å¾æå–: ä»2Då…³é”®ç‚¹è‡ªåŠ¨å­¦ä¹ ")
    
    def _create_transformer_layer(self, embed_dim, num_heads, dropout):
        """åˆ›å»ºè½»é‡Transformerå±‚"""
        return nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 2,  # å°FFN
            dropout=dropout,
            batch_first=True,
            activation='relu',  # ä½¿ç”¨ReLUèŠ‚çœæ˜¾å­˜
            norm_first=True  # å…ˆå½’ä¸€åŒ–æ›´ç¨³å®š
        )
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # çº¿æ€§å±‚ä½¿ç”¨Xavieråˆå§‹åŒ–
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        
        # ä½ç½®ç¼–ç å·²éšæœºåˆå§‹åŒ–
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥: 
            x: (B, T, J, 2) - 2Då…³èŠ‚ç‚¹åæ ‡
        è¾“å‡º: (B, T, J, 3) - 3Då…³èŠ‚ç‚¹åæ ‡
        """
        batch_size, seq_len, num_joints, _ = x.shape
        
        # æ£€æŸ¥è¾“å…¥åºåˆ—é•¿åº¦
        if seq_len > self.seq_len:
            raise ValueError(f"è¾“å…¥åºåˆ—é•¿åº¦{seq_len}è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦{self.seq_len}")
        
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºç‰¹å¾æå–
        x_input = x.clone()
        
        # 1. å…³èŠ‚ç‰¹å¾åµŒå…¥
        x = self.joint_embed(x)  # (B, T, J, D)
        
        # 2. ä»2Då…³é”®ç‚¹æå–å…¨å±€å½¢æ€ç‰¹å¾
        # å¯¹æ—¶é—´ç»´åº¦å–å¹³å‡ä»¥è·å¾—ç¨³å®šçš„å§¿æ€è¡¨ç¤º
        x_flat = x_input.view(batch_size, seq_len, -1)  # (B, T, J*2)
        pose_feat = self.pose_feature_extractor(x_flat.mean(dim=1))  # (B, D)
        pose_feat = pose_feat.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, D)
        x = x + pose_feat  # æ·»åŠ åˆ°å…³èŠ‚åµŒå…¥ä¸­
        
        # 3. æ·»åŠ ä½ç½®ç¼–ç  (å¹¿æ’­)
        x = x + self.time_pos_embed[:, :seq_len, :, :]
        x = x + self.joint_pos_embed[:, :, :num_joints, :]
        
        # 4. é‡å¡‘ä¸ºåºåˆ—: (B, T*J, D)
        x = x.reshape(batch_size, seq_len * num_joints, self.embed_dim)
        
        # 5. é€šè¿‡Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x)
        
        # 6. å±‚å½’ä¸€åŒ–
        x = self.norm(x)
        
        # 7. æ¢å¤åŸå§‹å½¢çŠ¶
        x = x.reshape(batch_size, seq_len, num_joints, self.embed_dim)
        
        # 8. è¾“å‡º3Dåæ ‡
        x = self.output_proj(x)
        
        return x



def test_model_memory():
    """æµ‹è¯•æ¨¡å‹æ˜¾å­˜å ç”¨"""
    import time
    
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹æ˜¾å­˜å ç”¨...")
    
    # æµ‹è¯•é…ç½®
    batch_sizes = [2, 4, 8]
    seq_lens = [8, 16]
    
    for seq_len in seq_lens:
        for batch_size in batch_sizes:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            try:
                # åˆ›å»ºæ¨¡å‹
                model = AnimalPoseTransformer(
                    num_joints=17,
                    embed_dim=96,
                    seq_len=seq_len
                ).cuda()
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(batch_size, seq_len, 17, 2).cuda()
                
                # å‰å‘ä¼ æ’­
                output = model(test_input)
                
                # åˆ›å»ºè™šæ‹ŸæŸå¤±å¹¶åå‘ä¼ æ’­
                loss = output.mean()
                loss.backward()
                
                # è·å–æ˜¾å­˜ä½¿ç”¨
                peak_memory = torch.cuda.max_memory_allocated() / (1024**2)
                allocated = torch.cuda.memory_allocated() / (1024**2)
                reserved = torch.cuda.memory_reserved() / (1024**2)
                
                print(f"  batch={batch_size}, seq={seq_len}: "
                      f"å³°å€¼{peak_memory:.1f}MB, "
                      f"å·²åˆ†é…{allocated:.1f}MB, "
                      f"ä¿ç•™{reserved:.1f}MB")
                
                del model, test_input, output, loss
                torch.cuda.empty_cache()
                
            except torch.cuda.OutOfMemoryError:
                print(f"  âŒ batch={batch_size}, seq={seq_len}: OOMé”™è¯¯")
            
            time.sleep(0.5)
    
    print("æµ‹è¯•å®Œæˆ!\n")


if __name__ == '__main__':
    # è¿è¡Œæ˜¾å­˜æµ‹è¯•
    if torch.cuda.is_available():
        test_model_memory()
    else:
        print("æ²¡æœ‰å¯ç”¨çš„GPUï¼Œè·³è¿‡æ˜¾å­˜æµ‹è¯•")