# video_to_3d_keypoints.py
import os
import cv2
import numpy as np
import torch
from tqdm import tqdm
import json
from common.ap10k_detector import AP10KAnimalPoseDetector
from common.keypoint_mapper import KeypointMapper
from common.model import TemporalModel
from common.camera import normalize_screen_coordinates


class VideoTo3DKeypoints:
    def __init__(self, model_checkpoint, onnx_model_path, output_dir="npz/estimate_npz",
                 architecture="3,3,3,3", channels=512, causal=False, dropout=0.2):
        """
        åˆå§‹åŒ–è§†é¢‘åˆ°3Då…³é”®ç‚¹è½¬æ¢å™¨
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹å‚æ•°
        self.architecture = architecture
        self.channels = channels
        self.causal = causal
        self.dropout = dropout

        # åˆå§‹åŒ–ç»„ä»¶
        self.detector = AP10KAnimalPoseDetector(onnx_model_path)
        self.mapper = KeypointMapper()

        # åŠ è½½3Dæ¨¡å‹
        self.model_3d = self.load_3d_model(model_checkpoint)

        print("âœ… è§†é¢‘åˆ°3Då…³é”®ç‚¹è½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def load_3d_model(self, checkpoint_path):
        """åŠ è½½è®­ç»ƒå¥½çš„3Då§¿æ€ä¼°è®¡æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½3Dæ¨¡å‹: {checkpoint_path}")

        # åˆ›å»ºæ¨¡å‹æ¶æ„
        filter_widths = [int(x) for x in self.architecture.split(',')]
        print(f"æ¨¡å‹æ¶æ„: {filter_widths}, é€šé“æ•°: {self.channels}")

        model = TemporalModel(
            17, 2, 17,
            filter_widths=filter_widths,
            causal=self.causal,
            dropout=self.dropout,
            channels=self.channels
        )

        # åŠ è½½æƒé‡
        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)

        if 'model_pos' in checkpoint:
            model_weights = checkpoint['model_pos']
            model.load_state_dict(model_weights)
            print("âœ… ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹æƒé‡æˆåŠŸ")
        else:
            print("âŒ æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ‰¾åˆ° 'model_pos' é”®")
            return None

        if torch.cuda.is_available():
            model = model.cuda()
            print("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
        else:
            print("â„¹ï¸ ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")

        model.eval()

        # è®¡ç®—æ„Ÿå—é‡
        self.receptive_field = model.receptive_field()
        self.pad = (self.receptive_field - 1) // 2
        print(f"æ¨¡å‹æ„Ÿå—é‡: {self.receptive_field}å¸§, å¡«å……: {self.pad}å¸§")

        # è®¡ç®—æœ€å°è¾“å…¥é•¿åº¦
        self.min_input_length = self.receptive_field
        print(f"æœ€å°è¾“å…¥åºåˆ—é•¿åº¦: {self.min_input_length}å¸§")

        return model

    def extract_2d_keypoints_from_video(self, video_path, confidence_threshold=0.3):
        """
        ä»è§†é¢‘ä¸­æå–2Då…³é”®ç‚¹

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            confidence_threshold: å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼
        """
        print(f"ğŸ¥ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"  è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}FPS")

        keypoints_2d_sequence = []
        valid_frames = 0
        frame_info = []

        # è¿›åº¦æ¡ - ä½¿ç”¨å®é™…æ€»å¸§æ•°
        pbar = tqdm(total=total_frames, desc="æå–2Då…³é”®ç‚¹")

        for frame_idx in range(total_frames):
            ret, frame = cap.read()
            if not ret:
                break

            # ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶ç”¨äºæ£€æµ‹
            temp_img_path = f"temp_frame_{frame_idx:06d}.jpg"
            cv2.imwrite(temp_img_path, frame)

            try:
                # ä½¿ç”¨AP10Kæ£€æµ‹å™¨è·å–2Då…³é”®ç‚¹
                result = self.detector.predict(temp_img_path)
                keypoints_ap10k = result['keypoints']

                # è¿‡æ»¤ä½ç½®ä¿¡åº¦å…³é”®ç‚¹
                valid_keypoints = np.sum(keypoints_ap10k[:, 2] > confidence_threshold)

                if valid_keypoints >= 8:
                    # æ˜ å°„åˆ°è®­ç»ƒæ¨¡å‹æ ¼å¼
                    keypoints_training = self.mapper.map_ap10k_to_training(keypoints_ap10k)

                    # æ·»åŠ åˆ°åºåˆ—
                    keypoints_2d_sequence.append(keypoints_training)
                    frame_info.append({
                        'frame_idx': frame_idx,
                        'valid_keypoints': valid_keypoints,
                        'timestamp': frame_idx / fps
                    })
                    valid_frames += 1

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_img_path)

            except Exception as e:
                if frame_idx % 100 == 0:
                    print(f"âš ï¸ å¸§ {frame_idx} å¤„ç†å¤±è´¥: {e}")
                if os.path.exists(temp_img_path):
                    os.remove(temp_img_path)

            pbar.update(1)

        pbar.close()
        cap.release()

        print(f"âœ… 2Då…³é”®ç‚¹æå–å®Œæˆ: {valid_frames}/{total_frames} æœ‰æ•ˆå¸§")

        if valid_frames < self.min_input_length:
            print(f"âŒ æœ‰æ•ˆå¸§æ•° ({valid_frames}) å°äºæ¨¡å‹è¦æ±‚çš„æœ€å°å¸§æ•° ({self.min_input_length})")
            print("ğŸ’¡ å»ºè®®ä½¿ç”¨æ›´é•¿çš„è§†é¢‘")
            return None

        return {
            'keypoints_2d': np.array(keypoints_2d_sequence),
            'frame_info': frame_info,
            'video_info': {
                'fps': fps,
                'total_frames': total_frames,
                'valid_frames': valid_frames,
                'video_path': video_path
            }
        }

    def convert_2d_to_3d(self, keypoints_2d_sequence):
        """
        å°†2Då…³é”®ç‚¹åºåˆ—è½¬æ¢ä¸º3Då…³é”®ç‚¹
        """
        print("ğŸ”„ å¼€å§‹2Dåˆ°3Dè½¬æ¢...")

        if len(keypoints_2d_sequence) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„2Då…³é”®ç‚¹æ•°æ®")

        # æ£€æŸ¥åºåˆ—é•¿åº¦
        seq_length = len(keypoints_2d_sequence)
        print(f"è¾“å…¥åºåˆ—é•¿åº¦: {seq_length}å¸§")

        if seq_length < self.min_input_length:
            print(f"âŒ åºåˆ—é•¿åº¦ä¸è¶³ï¼Œæ— æ³•å¤„ç†")
            return np.array([])

        # å½’ä¸€åŒ–2Dåæ ‡
        keypoints_2d_normalized = []
        for kp_2d in keypoints_2d_sequence:
            kp_normalized = normalize_screen_coordinates(kp_2d, w=1000, h=1000)
            keypoints_2d_normalized.append(kp_normalized)

        keypoints_2d_normalized = np.array(keypoints_2d_normalized)
        print(f"å½’ä¸€åŒ–å2Då…³é”®ç‚¹å½¢çŠ¶: {keypoints_2d_normalized.shape}")

        # ç›´æ¥å¤„ç†æ•´ä¸ªåºåˆ—ï¼Œä¸è¿›è¡Œæ»‘åŠ¨çª—å£
        all_3d_keypoints = []

        with torch.no_grad():
            # ç›´æ¥å¤„ç†æ•´ä¸ªåºåˆ—
            inputs_2d = torch.from_numpy(keypoints_2d_normalized.astype('float32')).unsqueeze(0)
            if torch.cuda.is_available():
                inputs_2d = inputs_2d.cuda()

            predicted_3d = self.model_3d(inputs_2d)
            keypoints_3d = predicted_3d.squeeze(0).cpu().numpy()
            all_3d_keypoints.append(keypoints_3d)

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        if all_3d_keypoints:
            keypoints_3d = np.concatenate(all_3d_keypoints, axis=0)
        else:
            keypoints_3d = np.array([])

        print(f"âœ… 3Dè½¬æ¢å®Œæˆ: {keypoints_3d.shape}")
        return keypoints_3d

    def save_to_npz(self, keypoints_3d, video_info, output_filename="data_3d_animals.npz"):
        """
        ä¿å­˜ä¸ºä¸è®­ç»ƒæ•°æ®ç›¸åŒæ ¼å¼çš„NPZæ–‡ä»¶
        """
        if len(keypoints_3d) == 0:
            print("âŒ æ²¡æœ‰3Då…³é”®ç‚¹æ•°æ®å¯ä¿å­˜")
            return None

        # åˆ›å»ºä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„ç»“æ„
        positions_3d = {
            'Animal': {
                'video_action': keypoints_3d
            }
        }

        # ä¿å­˜ä¸ºnpz
        output_path = os.path.join(self.output_dir, output_filename)
        np.savez_compressed(output_path, positions_3d=positions_3d)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'video_info': video_info,
            'keypoint_format': 'AP10K mapped to training format',
            'num_frames': len(keypoints_3d),
            'num_joints': 17,
            'processing_date': str(np.datetime64('now')),
            'model_architecture': self.architecture,
            'model_channels': self.channels,
            'receptive_field': self.receptive_field
        }

        metadata_path = os.path.join(self.output_dir, "processing_metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"ğŸ’¾ 3Då…³é”®ç‚¹å·²ä¿å­˜: {output_path}")
        print(f"   - å¸§æ•°: {len(keypoints_3d)}")
        print(f"   - å½¢çŠ¶: {keypoints_3d.shape}")

        return output_path

    def process_video(self, video_path, confidence_threshold=0.3):
        """
        å®Œæ•´å¤„ç†æµç¨‹ï¼šè§†é¢‘ -> 2Då…³é”®ç‚¹ -> 3Då…³é”®ç‚¹ -> NPZæ–‡ä»¶

        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            confidence_threshold: å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´å¤„ç†æµç¨‹...")
        print(f"æ¨¡å‹è¦æ±‚: è‡³å°‘ {self.min_input_length} å¸§è¾“å…¥")

        try:
            # 1. æå–2Då…³é”®ç‚¹ï¼ˆå¤„ç†æ•´ä¸ªè§†é¢‘ï¼‰
            extraction_result = self.extract_2d_keypoints_from_video(
                video_path, confidence_threshold
            )

            if extraction_result is None:
                return None

            # 2. è½¬æ¢ä¸º3D
            keypoints_3d = self.convert_2d_to_3d(extraction_result['keypoints_2d'])

            if len(keypoints_3d) == 0:
                print("âŒ 3Dè½¬æ¢å¤±è´¥")
                return None

            # 3. ä¿å­˜ä¸ºNPZ
            output_path = self.save_to_npz(
                keypoints_3d,
                extraction_result['video_info']
            )

            print("ğŸ‰ å¤„ç†å®Œæˆ!")
            return output_path

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    # é…ç½®è·¯å¾„
    MODEL_CHECKPOINT = "checkpoint/epoch_100.bin"
    ONNX_MODEL_PATH = "model/ap10k/end2end.onnx"
    VIDEO_PATH = "video/test_video.mp4"

    # åˆ›å»ºå¤„ç†å™¨
    processor = VideoTo3DKeypoints(
        model_checkpoint=MODEL_CHECKPOINT,
        onnx_model_path=ONNX_MODEL_PATH,
        architecture="3,3,3,3",
        channels=512,
        causal=False,
        dropout=0.2
    )

    # å¤„ç†æ•´ä¸ªè§†é¢‘ï¼Œä¸é™åˆ¶å¸§æ•°
    output_npz = processor.process_video(
        VIDEO_PATH,
        confidence_threshold=0.2
    )

    if output_npz:
        print(f"âœ… å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_npz}")

        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        try:
            data = np.load(output_npz, allow_pickle=True)
            positions_3d = data['positions_3d'].item()
            print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶éªŒè¯:")
            for subject, actions in positions_3d.items():
                for action, keypoints in actions.items():
                    print(f"   {subject}/{action}: {keypoints.shape}")
        except Exception as e:
            print(f"âš ï¸ è¾“å‡ºæ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
    else:
        print("âŒ å¤„ç†å¤±è´¥")


if __name__ == "__main__":
    main()